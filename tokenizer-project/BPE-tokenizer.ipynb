{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIexuHRrFmLGXWwXinpWS1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Piranavan25/ml-portfolio/blob/main/tokenizer-project/BPE-tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usja35MgrtVC"
      },
      "outputs": [],
      "source": [
        "!pip install datasets tokenizers matplotlib --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders\n",
        "import re"
      ],
      "metadata": {
        "id": "j1WaWxCVr0G1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading Wikitext-103...\")\n",
        "\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-103-v1\", split=\"train\")\n",
        "\n",
        "texts = [t[\"text\"] for t in dataset if t[\"text\"].strip()]\n",
        "\n",
        "print(\"Total samples:\", len(texts))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iCsD0nItCsb",
        "outputId": "18c569fe-c0f3-4e55-dae9-eb15d3aa9e84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Wikitext-103...\n",
            "Total samples: 1165029\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return text.strip()\n",
        "\n",
        "texts = [clean_text(t) for t in texts]\n"
      ],
      "metadata": {
        "id": "zq9FtpZkuTJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(models.BPE())\n",
        "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)\n",
        "\n",
        "trainer = trainers.BpeTrainer(\n",
        "    vocab_size=30000,\n",
        "    min_frequency=2,\n",
        "    special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"]\n",
        ")\n",
        "\n",
        "print(\"Training tokenizer...\")\n",
        "tokenizer.train_from_iterator(texts, trainer=trainer)\n",
        "tokenizer.decoder = decoders.ByteLevel()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjerf_rKulH8",
        "outputId": "06af0e92-5f34-4adb-ca40-25b3d9ea32dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training tokenizer...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save(\"wikitext_tokenizer.json\")\n",
        "print(\"Tokenizer saved as wikitext_tokenizer.json\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcDKc_avuxzp",
        "outputId": "e5f5e47e-1e63-49df-81cb-f69e0ba7211e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer saved as wikitext_tokenizer.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded = tokenizer.encode(\"Hello, this is my custom tokenizer!\")\n",
        "print(encoded.tokens)\n",
        "print(encoded.ids)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKTwCiOqu738",
        "outputId": "2ad79f89-0cf0-43a5-97be-e2de3a5499cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ĠHello', ',', 'Ġthis', 'Ġis', 'Ġmy', 'Ġcustom', 'Ġtoken', 'izer', '!']\n",
            "[24091, 16, 549, 318, 1542, 8229, 29907, 11076, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EWHnW701vmjN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}